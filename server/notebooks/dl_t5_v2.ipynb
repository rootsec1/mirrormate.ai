{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Config\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sequence</th>\n",
       "      <th>target_move</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>d4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d4 d6</td>\n",
       "      <td>Nf3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d4 d6 Nf3 Nf6</td>\n",
       "      <td>Bg5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4 d6 Nf3 Nf6 Bg5 c6</td>\n",
       "      <td>c4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d4 d6 Nf3 Nf6 Bg5 c6 c4 Qa5+</td>\n",
       "      <td>Nc3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8431</th>\n",
       "      <td>e4 c5</td>\n",
       "      <td>Nf3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8432</th>\n",
       "      <td>e4 c5 Nf3 d6</td>\n",
       "      <td>Bc4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>e4 c5 Nf3 d6 Bc4 Nc6</td>\n",
       "      <td>d3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>e4 c5 Nf3 d6 Bc4 Nc6 d3 Nf6</td>\n",
       "      <td>O-O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>e4 c5 Nf3 d6 Bc4 Nc6 d3 Nf6 O-O Bd7</td>\n",
       "      <td>Bg5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8436 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           input_sequence target_move\n",
       "0                                                  d4\n",
       "1                                   d4 d6         Nf3\n",
       "2                           d4 d6 Nf3 Nf6         Bg5\n",
       "3                    d4 d6 Nf3 Nf6 Bg5 c6          c4\n",
       "4            d4 d6 Nf3 Nf6 Bg5 c6 c4 Qa5+         Nc3\n",
       "...                                   ...         ...\n",
       "8431                                e4 c5         Nf3\n",
       "8432                         e4 c5 Nf3 d6         Bc4\n",
       "8433                 e4 c5 Nf3 d6 Bc4 Nc6          d3\n",
       "8434          e4 c5 Nf3 d6 Bc4 Nc6 d3 Nf6         O-O\n",
       "8435  e4 c5 Nf3 d6 Bc4 Nc6 d3 Nf6 O-O Bd7         Bg5\n",
       "\n",
       "[8436 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "lichess_username = \"ruchitoshniwal1\"\n",
    "df = pd.read_csv(f\"../data/processed/sequence_target_map_{lichess_username}.csv\")\n",
    "df.fillna(\"\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8  # Adjust batch size according to your GPU memory\n",
    "ML_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 858636\n"
     ]
    }
   ],
   "source": [
    "def create_san_vocabulary():\n",
    "    pieces = ['P', 'N', 'B', 'R', 'Q', 'K', '', 'p', 'n', 'b', 'r', 'q', 'k']\n",
    "    columns = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    ranks = ['1', '2', '3', '4', '5', '6', '7', '8']\n",
    "    special = ['O-O', 'O-O-O', '+', '#', 'x', '=', 'e.p.']\n",
    "\n",
    "    vocabulary = set()\n",
    "    for piece in pieces:\n",
    "        for col1 in columns:\n",
    "            for rank1 in ranks:\n",
    "                for col2 in columns:\n",
    "                    for rank2 in ranks:\n",
    "                        # Add moves like \"e4\", \"Nf3\", \"Bb5+\", etc.\n",
    "                        line1 = f\"{piece}{col1}{rank1}{col2}{rank2}\"\n",
    "                        line2 = f\"{piece}{col1}{rank1}x{col2}{rank2}\"\n",
    "                        line3 = f\"{piece}{col2}{rank2}\"\n",
    "                        line4 = f\"{col2}{rank2}\"\n",
    "                        \n",
    "                        vocabulary.add(line1)\n",
    "                        vocabulary.add(line2)\n",
    "                        vocabulary.add(line3)\n",
    "                        vocabulary.add(line4)\n",
    "\n",
    "                        for sp in special:\n",
    "                            # Add moves like \"O-O\", \"O-O-O\", \"Nf3#\", etc.\n",
    "                            line5 = f\"{line1}{sp}\"\n",
    "                            line6 = f\"{line2}{sp}\"\n",
    "                            line7 = f\"{line3}{sp}\"\n",
    "                            line8 = f\"{line4}{sp}\"\n",
    "                            \n",
    "                            vocabulary.add(line5)\n",
    "                            vocabulary.add(line6)\n",
    "                            vocabulary.add(line7)\n",
    "                            vocabulary.add(line8)\n",
    "                # Add simpler moves like \"e4\", \"Nf3\", etc.\n",
    "                vocabulary.add(f\"{piece}{col1}{rank1}\")\n",
    "    # Add special moves and annotations\n",
    "    vocabulary.update(special)\n",
    "\n",
    "    # Convert the set to a list to fix the order\n",
    "    vocabulary = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'] + list(vocabulary)  # Add special tokens\n",
    "    return vocabulary\n",
    "\n",
    "# Create the vocabulary\n",
    "custom_vocabulary = create_san_vocabulary()\n",
    "print(f\"Vocabulary size: {len(custom_vocabulary)}\")\n",
    "# Define custom tokenizer class\n",
    "class SimpleChessTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = {v: k for k, v in enumerate(vocab)}\n",
    "        self.reverse_vocab = {k: v for v, k in self.vocab.items()}\n",
    "\n",
    "    def encode(self, moves):\n",
    "        return [self.vocab.get(move, self.vocab['[UNK]']) for move in moves]\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return [self.reverse_vocab.get(token_id, '[UNK]') for token_id in token_ids]\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = SimpleChessTokenizer(custom_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1] at entry 0 and [2] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Assuming you have defined train and test functions\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):  \u001b[38;5;66;03m# Assuming 4 epochs\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     avg_training_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[+] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m average training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_training_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m     val_loss, val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader)\n",
      "Cell \u001b[0;32mIn[27], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer)\u001b[0m\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     56\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     58\u001b[0m     sequences, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Pass sequences and targets through your model, compute loss, perform backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Code/ML/mirrormate.ai/server/env/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1] at entry 0 and [2] at entry 1"
     ]
    }
   ],
   "source": [
    "# Define the chess dataset\n",
    "class ChessMovesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets, tokenizer):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoded_seq = self.tokenizer.encode(self.sequences[idx])\n",
    "        encoded_target = self.tokenizer.encode([self.targets[idx]])[0]  # Encode target move\n",
    "        return torch.tensor(encoded_seq, dtype=torch.long), torch.tensor(encoded_target, dtype=torch.long)\n",
    "\n",
    "# Assuming 'sequences' is a list of lists of SAN moves, 'targets' is a list of SAN moves.\n",
    "# Here's how you might extract them from your DataFrame:\n",
    "sequences = [row[\"input_sequence\"].split(\" \") for index, row in df.iterrows()]\n",
    "targets = df.target_move.tolist()\n",
    "\n",
    "\n",
    "TRAINING_SET_SIZE = 0.8\n",
    "\n",
    "# Splitting data into training and testing sets for example\n",
    "split = int(len(sequences) * TRAINING_SET_SIZE)\n",
    "train_sequences, train_targets = sequences[:split], targets[:split]\n",
    "test_sequences, test_targets = sequences[split:], targets[split:]\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChessMovesDataset(train_sequences, train_targets, tokenizer)\n",
    "test_dataset = ChessMovesDataset(test_sequences, test_targets, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# T5 configuration\n",
    "config = T5Config(\n",
    "    vocab_size=len(custom_vocabulary),\n",
    "    d_model=MODEL_MAX_LENGTH,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    num_heads=8,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "model = T5ForConditionalGeneration(config=config).to(ML_DEVICE)\n",
    "\n",
    "# Training settings\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Adjust if necessary for your specific task\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Train function (simplified for brevity)\n",
    "def train(model, loader, optimizer):\n",
    "    TRAIN_LOADER_LENGTH = len(loader)\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        sequences, targets = batch\n",
    "        # Pass sequences and targets through your model, compute loss, perform backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / TRAIN_LOADER_LENGTH\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    # Evaluation Loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total, correct = 0, 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # No need to track gradients during evaluation\n",
    "        for batch in test_loader:\n",
    "            sequences, targets = batch\n",
    "            outputs = model(sequences)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            \n",
    "    total_loss = total_loss / len(loader)\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Assuming you have defined train and test functions\n",
    "for epoch in range(NUM_EPOCHS):  # Assuming 4 epochs\n",
    "    avg_training_loss = train(model, train_loader, optimizer)\n",
    "    print(f'[+] Epoch {epoch + 1} average training loss: {avg_training_loss}')\n",
    "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
    "    print(f'Validation Loss: {val_loss}')\n",
    "    print(f'Validation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(f\"../models/t5/{lichess_username}_t5_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
